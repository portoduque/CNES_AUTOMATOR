#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Automatizador da API CNES - VERS√ÉO ASS√çNCRONA OTIMIZADA COM LOADING

Este script automatiza o consumo da API p√∫blica do CNES para obter dados
detalhados de estabelecimentos de sa√∫de usando seus c√≥digos CNES espec√≠ficos.

OTIMIZA√á√ïES IMPLEMENTADAS:
- Processamento ass√≠ncrono com requisi√ß√µes simult√¢neas
- Sistema de loading em tempo real com ETA
- Pool de conex√µes otimizado
- Backup incremental dos dados

Autor: Script Automatizado
Data: 2025
API: https://apidadosabertos.saude.gov.br/cnes/estabelecimentos/{codigo_cnes}
"""

import json
import asyncio
import aiohttp
import time
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
import logging
import sys

# Configura√ß√£o de logging para acompanhar o progresso
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cnes_automator.log'),
        logging.StreamHandler()
    ]
)

class ProgressTracker:
    """
    Classe para rastrear e exibir progresso em tempo real - VERS√ÉO APRIMORADA
    """
    
    def __init__(self, total_items: int, description: str = "Processando"):
        self.total_items = total_items
        self.processed_items = 0
        self.description = description
        self.start_time = time.time()
        self.last_update = 0
        self.success_count = 0
        self.error_count = 0
        self.last_rates = []  # Para calcular velocidade m√©dia dos √∫ltimos segundos
        
    def update(self, processed: int, current_batch: int = None, total_batches: int = None, 
               success_count: int = None, error_count: int = None):
        """
        Atualiza o progresso e exibe informa√ß√µes detalhadas
        """
        self.processed_items = processed
        current_time = time.time()
        
        # Atualiza contadores se fornecidos
        if success_count is not None:
            self.success_count = success_count
        if error_count is not None:
            self.error_count = error_count
        
        # Atualiza a cada 0.3 segundos para feedback mais frequente
        if current_time - self.last_update < 0.3 and processed < self.total_items:
            return
            
        self.last_update = current_time
        
        # Calcula estat√≠sticas
        elapsed_time = current_time - self.start_time
        remaining_items = self.total_items - processed
        
        if elapsed_time > 0:
            current_rate = processed / elapsed_time
            
            # Mant√©m hist√≥rico das √∫ltimas 10 medi√ß√µes para velocidade mais est√°vel
            self.last_rates.append(current_rate)
            if len(self.last_rates) > 10:
                self.last_rates.pop(0)
            
            # Velocidade m√©dia das √∫ltimas medi√ß√µes
            avg_rate = sum(self.last_rates) / len(self.last_rates)
            
            if avg_rate > 0:
                eta_seconds = remaining_items / avg_rate
                eta = str(timedelta(seconds=int(eta_seconds)))
            else:
                eta = "Calculando..."
        else:
            avg_rate = 0
            eta = "Calculando..."
        
        # Calcula porcentagem
        percentage = (processed / self.total_items) * 100
        
        # Cria barra de progresso mais detalhada
        bar_length = 40
        filled_length = int(bar_length * processed // self.total_items)
        bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
        
        # Formata informa√ß√µes do lote se fornecidas
        batch_info = ""
        if current_batch is not None and total_batches is not None:
            batch_info = f" | Lote {current_batch}/{total_batches}"
        
        # Informa√ß√µes de sucesso/erro
        status_info = ""
        if self.success_count > 0 or self.error_count > 0:
            status_info = f" | ‚úÖ {self.success_count} ‚ùå {self.error_count}"
        
        # Estimativa de tempo formatada
        eta_display = eta if eta != "Calculando..." else "‚è≥ Calc..."
        
        # Monta a linha de progresso detalhada
        progress_line = (
            f"\r{self.description}: |{bar}| "
            f"{processed:,}/{self.total_items:,} ({percentage:.1f}%) "
            f"| Restam: {remaining_items:,} | {avg_rate:.1f}/s | ETA: {eta_display}{batch_info}{status_info}"
        )
        
        # Limita o tamanho da linha para evitar problemas no terminal
        if len(progress_line) > 120:
            progress_line = progress_line[:117] + "..."
        
        # Exibe o progresso
        sys.stdout.write(progress_line)
        sys.stdout.flush()
        
        # Se terminou, pula linha
        if processed >= self.total_items:
            print()  # Nova linha no final
    
    def finish(self):
        """
        Finaliza o progresso com estat√≠sticas detalhadas
        """
        elapsed_time = time.time() - self.start_time
        rate = self.total_items / elapsed_time if elapsed_time > 0 else 0
        
        print(f"\n‚úÖ {self.description} conclu√≠do!")
        print(f"üìä Total processado: {self.total_items:,}")
        print(f"‚è±Ô∏è Tempo total: {elapsed_time:.1f}s ({timedelta(seconds=int(elapsed_time))})")
        print(f"‚ö° Velocidade m√©dia: {rate:.1f} req/s")
        
        if self.success_count > 0 or self.error_count > 0:
            success_rate = (self.success_count / self.total_items) * 100 if self.total_items > 0 else 0
            print(f"‚úÖ Sucessos: {self.success_count:,} ({success_rate:.1f}%)")
            print(f"‚ùå Erros: {self.error_count:,} ({100-success_rate:.1f}%)")
        
        print("-" * 50)

class DateTimeEncoder(json.JSONEncoder):
    """
    Encoder customizado para serializar objetos datetime
    """
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

class CNESAPIAutomator:
    """
    Classe principal para automatizar consultas na API CNES - VERS√ÉO ASS√çNCRONA OTIMIZADA
    """
    
    def __init__(self, concurrent_requests: int = 10, delay_between_batches: float = 0.5):
        """
        Inicializa o automatizador ass√≠ncrono
        
        Args:
            concurrent_requests (int): N√∫mero de requisi√ß√µes simult√¢neas (padr√£o: 10)
            delay_between_batches (float): Tempo de espera entre lotes (padr√£o: 0.5s)
        """
        self.base_url = "https://apidadosabertos.saude.gov.br/cnes/estabelecimentos"
        self.concurrent_requests = concurrent_requests
        self.delay_between_batches = delay_between_batches
        
        # Headers para as requisi√ß√µes
        self.headers = {
            'User-Agent': 'CNES-Automator/2.0-AsyncOptimized-WithProgress',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        }
        
        # Estat√≠sticas da execu√ß√£o
        self.stats = {
            'total_requisicoes': 0,
            'sucessos': 0,
            'erros': 0,
            'codigos_invalidos': 0,
            'erros_conexao': 0,
            'inicio_execucao': None,
            'fim_execucao': None
        }

    def carregar_codigos_cnes(self, arquivo_entrada: str) -> List[str]:
        """
        Carrega a lista de c√≥digos CNES de um arquivo JSON
        
        Args:
            arquivo_entrada (str): Caminho para o arquivo JSON com os c√≥digos
            
        Returns:
            List[str]: Lista de c√≥digos CNES
        """
        logging.info(f"üìÇ Carregando c√≥digos CNES do arquivo: {arquivo_entrada}")
        
        try:
            with open(arquivo_entrada, 'r', encoding='utf-8') as arquivo:
                dados = json.load(arquivo)
            
            # Extrai os c√≥digos CNES do arquivo
            codigos = []
            
            # Se for uma lista de objetos com campo 'codigo_cnes'
            if isinstance(dados, list):
                for item in dados:
                    if isinstance(item, dict) and 'codigo_cnes' in item:
                        codigos.append(str(item['codigo_cnes']))
                    elif isinstance(item, (str, int)):
                        codigos.append(str(item))
            
            # Se for um objeto com campo 'estabelecimentos'
            elif isinstance(dados, dict):
                if 'estabelecimentos' in dados:
                    for estabelecimento in dados['estabelecimentos']:
                        if 'codigo_cnes' in estabelecimento:
                            codigos.append(str(estabelecimento['codigo_cnes']))
                elif 'codigo_cnes' in dados:
                    codigos.append(str(dados['codigo_cnes']))
                elif 'codigos' in dados:
                    codigos.extend([str(codigo) for codigo in dados['codigos']])
            
            # Remove duplicatas e c√≥digos vazios
            codigos = list(set([codigo for codigo in codigos if codigo.strip()]))
            
            logging.info(f"‚úÖ Carregados {len(codigos)} c√≥digos CNES √∫nicos")
            
            if not codigos:
                raise ValueError("Nenhum c√≥digo CNES v√°lido encontrado no arquivo")
            
            return codigos
            
        except Exception as e:
            logging.error(f"‚ùå Erro ao carregar arquivo: {e}")
            raise

    async def consultar_estabelecimento_async(self, session: aiohttp.ClientSession, codigo_cnes: str) -> Tuple[bool, Dict[str, Any]]:
        """
        Consulta um estabelecimento espec√≠fico na API CNES de forma ass√≠ncrona
        
        Args:
            session (aiohttp.ClientSession): Sess√£o HTTP ass√≠ncrona
            codigo_cnes (str): C√≥digo CNES do estabelecimento
            
        Returns:
            Tuple[bool, Dict]: (sucesso, dados_ou_erro)
        """
        url = f"{self.base_url}/{codigo_cnes}"
        
        try:
            # Incrementa contador de requisi√ß√µes
            self.stats['total_requisicoes'] += 1
            
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status == 200:
                    try:
                        dados = await response.json()
                        
                        # Adiciona metadados ao estabelecimento
                        if isinstance(dados, dict):
                            dados['_metadata'] = {
                                'consultado_em': datetime.now().isoformat(),
                                'codigo_cnes_consultado': codigo_cnes,
                                'url_consultada': url
                            }
                        
                        self.stats['sucessos'] += 1
                        return True, dados
                        
                    except Exception as json_error:
                        erro = {
                            'codigo_cnes': codigo_cnes,
                            'erro': 'Resposta n√£o √© um JSON v√°lido',
                            'status_code': response.status,
                            'detalhes': str(json_error)
                        }
                        self.stats['erros'] += 1
                        return False, erro
                        
                elif response.status == 404:
                    erro = {
                        'codigo_cnes': codigo_cnes,
                        'erro': 'C√≥digo CNES n√£o encontrado',
                        'status_code': 404,
                        'url_consultada': url
                    }
                    self.stats['codigos_invalidos'] += 1
                    return False, erro
                    
                else:
                    erro = {
                        'codigo_cnes': codigo_cnes,
                        'erro': f'Erro HTTP {response.status}',
                        'status_code': response.status,
                        'url_consultada': url
                    }
                    self.stats['erros'] += 1
                    return False, erro
                    
        except asyncio.TimeoutError:
            erro = {
                'codigo_cnes': codigo_cnes,
                'erro': 'Timeout na requisi√ß√£o',
                'detalhes': 'A requisi√ß√£o demorou mais que 15 segundos',
                'url_consultada': url
            }
            self.stats['erros_conexao'] += 1
            return False, erro
            
        except Exception as e:
            erro = {
                'codigo_cnes': codigo_cnes,
                'erro': 'Erro inesperado',
                'detalhes': str(e),
                'url_consultada': url
            }
            self.stats['erros'] += 1
            return False, erro

    async def processar_lote_codigos(self, session: aiohttp.ClientSession, codigos_lote: List[str]) -> List[Tuple[bool, Dict[str, Any]]]:
        """
        Processa um lote de c√≥digos CNES de forma ass√≠ncrona
        """
        tarefas = []
        for codigo in codigos_lote:
            tarefa = self.consultar_estabelecimento_async(session, codigo)
            tarefas.append(tarefa)
        
        # Executa todas as tarefas do lote simultaneamente
        resultados = await asyncio.gather(*tarefas, return_exceptions=True)
        
        # Processa exce√ß√µes
        resultados_limpos = []
        for i, resultado in enumerate(resultados):
            if isinstance(resultado, Exception):
                erro = {
                    'codigo_cnes': codigos_lote[i],
                    'erro': 'Exce√ß√£o durante processamento',
                    'detalhes': str(resultado)
                }
                resultados_limpos.append((False, erro))
            else:
                resultados_limpos.append(resultado)
        
        return resultados_limpos

    def salvar_backup_incremental(self, estabelecimentos: List[Dict], erros: List[Dict], arquivo_backup: str):
        """
        Salva backup incremental dos dados durante o processamento
        """
        try:
            dados_backup = {
                'timestamp_backup': datetime.now().isoformat(),
                'estabelecimentos_processados': len(estabelecimentos),
                'erros_encontrados': len(erros),
                'estabelecimentos': estabelecimentos,
                'erros': erros
            }
            
            with open(arquivo_backup, 'w', encoding='utf-8') as arquivo:
                json.dump(dados_backup, arquivo, ensure_ascii=False, indent=2, cls=DateTimeEncoder)
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Erro ao salvar backup: {e}")

    async def processar_lista_codigos(self, codigos_cnes: List[str]) -> Dict[str, Any]:
        """
        Processa uma lista de c√≥digos CNES de forma ass√≠ncrona otimizada com loading em tempo real
        
        Args:
            codigos_cnes (List[str]): Lista de c√≥digos CNES para consultar
            
        Returns:
            Dict[str, Any]: Dados consolidados com estabelecimentos e erros
        """
        # Exibe informa√ß√µes iniciais detalhadas
        print("=" * 60)
        print("üöÄ CNES AUTOMATOR - PROCESSAMENTO ASS√çNCRONO OTIMIZADO")
        print("=" * 60)
        print(f"üìã Total de c√≥digos CNES: {len(codigos_cnes):,}")
        print(f"‚ö° Requisi√ß√µes simult√¢neas: {self.concurrent_requests}")
        print(f"‚è±Ô∏è Delay entre lotes: {self.delay_between_batches}s")
        
        # Calcula estimativa inicial
        estimativa_tempo = len(codigos_cnes) / (self.concurrent_requests * (1/self.delay_between_batches))
        print(f"üîÆ Tempo estimado: ~{estimativa_tempo:.1f}s ({timedelta(seconds=int(estimativa_tempo))})")
        
        # Divide os c√≥digos em lotes
        lotes = []
        for i in range(0, len(codigos_cnes), self.concurrent_requests):
            lote = codigos_cnes[i:i + self.concurrent_requests]
            lotes.append(lote)
        
        print(f"üì¶ Dividido em {len(lotes)} lotes")
        print("=" * 60)
        
        logging.info(f"üöÄ Iniciando processamento ass√≠ncrono de {len(codigos_cnes)} c√≥digos CNES")
        logging.info(f"‚ö° Configura√ß√£o: {self.concurrent_requests} requisi√ß√µes simult√¢neas")
        
        self.stats['inicio_execucao'] = datetime.now().isoformat()
        
        # Estruturas para armazenar resultados
        estabelecimentos_validos = []
        erros_encontrados = []
        
        # Arquivo de backup incremental
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        arquivo_backup = f"cnes_backup_{timestamp}.json"
        
        logging.info(f"üì¶ Dividido em {len(lotes)} lotes para processamento")
        
        # Inicializa o tracker de progresso
        progress_tracker = ProgressTracker(len(codigos_cnes), "üè• Consultando API CNES")
        
        # Configura√ß√µes do connector para otimiza√ß√£o
        connector = aiohttp.TCPConnector(
            limit=self.concurrent_requests + 5,  # Pool de conex√µes
            limit_per_host=self.concurrent_requests,
            ttl_dns_cache=300,  # Cache DNS por 5 minutos
            use_dns_cache=True,
        )
        
        # Timeout personalizado
        timeout = aiohttp.ClientTimeout(total=15, connect=5)
        
        try:
            async with aiohttp.ClientSession(
                headers=self.headers,
                connector=connector,
                timeout=timeout
            ) as session:
                
                for i, lote in enumerate(lotes, 1):
                    # Processa o lote
                    resultados_lote = await self.processar_lote_codigos(session, lote)
                    
                    # Processa os resultados
                    for j, (sucesso, resultado) in enumerate(resultados_lote):
                        if sucesso:
                            if isinstance(resultado, dict) and '_metadata' in resultado:
                                resultado['_metadata']['indice_processamento'] = (i-1) * self.concurrent_requests + j + 1
                            estabelecimentos_validos.append(resultado)
                        else:
                            erros_encontrados.append(resultado)
                    
                    # Atualiza o progresso com informa√ß√µes detalhadas
                    processados = min(i * self.concurrent_requests, len(codigos_cnes))
                    progress_tracker.update(
                        processed=processados, 
                        current_batch=i, 
                        total_batches=len(lotes),
                        success_count=len(estabelecimentos_validos),
                        error_count=len(erros_encontrados)
                    )
                    
                    # Salva backup a cada 5 lotes
                    if i % 5 == 0:
                        self.salvar_backup_incremental(estabelecimentos_validos, erros_encontrados, arquivo_backup)
                    
                    # Pausa entre lotes (exceto no √∫ltimo)
                    if i < len(lotes):
                        await asyncio.sleep(self.delay_between_batches)
        
        except Exception as e:
            logging.error(f"‚ùå Erro durante sess√£o ass√≠ncrona: {e}")
            raise
        
        finally:
            # Fecha o connector
            await connector.close()
        
        # Finaliza o progresso
        progress_tracker.finish()
        
        self.stats['fim_execucao'] = datetime.now().isoformat()
        
        # Calcula tempo de execu√ß√£o
        inicio = datetime.fromisoformat(self.stats['inicio_execucao'])
        fim = datetime.fromisoformat(self.stats['fim_execucao'])
        tempo_execucao = (fim - inicio).total_seconds()
        
        # Consolida os resultados finais
        resultado_consolidado = {
            'metadados': {
                'data_processamento': self.stats['fim_execucao'],
                'tempo_execucao_segundos': tempo_execucao,
                'fonte_api': self.base_url,
                'total_codigos_processados': len(codigos_cnes),
                'versao_script': '2.0_async_optimized_with_progress',
                'configuracao_performance': {
                    'requisicoes_simultaneas': self.concurrent_requests,
                    'delay_entre_lotes': self.delay_between_batches,
                    'total_lotes': len(lotes)
                },
                'estatisticas': self.stats.copy()
            },
            'estabelecimentos': estabelecimentos_validos,
            'erros': erros_encontrados,
            'resumo': {
                'total_sucessos': len(estabelecimentos_validos),
                'total_erros': len(erros_encontrados),
                'taxa_sucesso': f"{(len(estabelecimentos_validos)/len(codigos_cnes)*100):.1f}%" if codigos_cnes else "0%",
                'velocidade_media': f"{len(codigos_cnes)/tempo_execucao:.1f} req/s" if tempo_execucao > 0 else "N/A"
            }
        }
        
        logging.info(f"‚úÖ Processamento ass√≠ncrono conclu√≠do!")
        logging.info(f"üìà Sucessos: {len(estabelecimentos_validos)}")
        logging.info(f"‚ùå Erros: {len(erros_encontrados)}")
        logging.info(f"üìä Taxa de sucesso: {resultado_consolidado['resumo']['taxa_sucesso']}")
        logging.info(f"‚ö° Velocidade m√©dia: {resultado_consolidado['resumo']['velocidade_media']}")
        logging.info(f"‚è±Ô∏è Tempo total: {tempo_execucao:.1f} segundos")
        
        # Remove arquivo de backup se processamento foi bem-sucedido
        try:
            if os.path.exists(arquivo_backup):
                os.remove(arquivo_backup)
                logging.info(f"üóëÔ∏è Backup tempor√°rio removido: {arquivo_backup}")
        except:
            pass
        
        return resultado_consolidado

    def salvar_resultados(self, dados: Dict[str, Any], arquivo_saida: str):
        """
        Salva os resultados consolidados em um arquivo JSON com valida√ß√£o
        
        Args:
            dados (Dict[str, Any]): Dados consolidados para salvar
            arquivo_saida (str): Caminho do arquivo de sa√≠da
        """
        try:
            logging.info(f"üíæ Salvando resultados em: {arquivo_saida}")
            
            # Primeiro, valida se os dados podem ser serializados
            json_string = json.dumps(dados, ensure_ascii=False, indent=2, cls=DateTimeEncoder)
            
            # Salva o arquivo
            with open(arquivo_saida, 'w', encoding='utf-8') as arquivo:
                arquivo.write(json_string)
            
            # Verifica se o arquivo foi salvo corretamente
            tamanho_arquivo = os.path.getsize(arquivo_saida)
            
            # Testa se o arquivo pode ser lido de volta
            with open(arquivo_saida, 'r', encoding='utf-8') as arquivo:
                dados_verificacao = json.load(arquivo)
            
            logging.info(f"‚úÖ Arquivo salvo e verificado com sucesso!")
            logging.info(f"üìÅ Tamanho: {tamanho_arquivo:,} bytes")
            logging.info(f"üìä Estabelecimentos salvos: {len(dados_verificacao.get('estabelecimentos', []))}")
            logging.info(f"‚ùå Erros salvos: {len(dados_verificacao.get('erros', []))}")
            
        except Exception as e:
            logging.error(f"‚ùå Erro ao salvar arquivo: {e}")
            
            # Tenta salvar um arquivo de emerg√™ncia sem formata√ß√£o
            try:
                arquivo_emergencia = arquivo_saida.replace('.json', '_emergencia.json')
                with open(arquivo_emergencia, 'w', encoding='utf-8') as arquivo:
                    json.dump(dados, arquivo, ensure_ascii=False, cls=DateTimeEncoder)
                logging.info(f"üíæ Arquivo de emerg√™ncia salvo: {arquivo_emergencia}")
            except Exception as e2:
                logging.error(f"‚ùå Falhou tamb√©m no arquivo de emerg√™ncia: {e2}")
            
            raise

class CNESMacrorregiaeMerger:
    """
    Classe para mesclar dados de macrorregi√£o com dados das unidades de sa√∫de
    """
    
    def __init__(self, arquivo_macrorregiao: str):
        """
        Inicializa o merger com o arquivo de macrorregi√µes
        
        Args:
            arquivo_macrorregiao (str): Caminho para o arquivo JSON com dados de macrorregi√£o
        """
        self.arquivo_macrorregiao = arquivo_macrorregiao
        self.dados_macrorregiao = {}
        self.carregar_dados_macrorregiao()
        
        # Configurar logging espec√≠fico para merger
        self.logger = logging.getLogger('CNESMacrorregiaeMerger')
        handler = logging.FileHandler('cnes_macrorregiao_merger.log')
        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def carregar_dados_macrorregiao(self):
        """
        Carrega os dados de macrorregi√£o do arquivo JSON e cria √≠ndice por c√≥digo de munic√≠pio
        """
        try:
            logging.info(f"üìÇ Carregando dados de macrorregi√£o de: {self.arquivo_macrorregiao}")
            
            with open(self.arquivo_macrorregiao, 'r', encoding='utf-8') as arquivo:
                dados = json.load(arquivo)
            
            # Verifica se a estrutura cont√©m o campo esperado
            if 'macrorregiao_regiao_saude_municipios' in dados:
                lista_municipios = dados['macrorregiao_regiao_saude_municipios']
            elif isinstance(dados, list):
                lista_municipios = dados
            else:
                raise ValueError("Estrutura do arquivo de macrorregi√£o n√£o reconhecida")
            
            # Cria √≠ndice por c√≥digo de munic√≠pio para busca r√°pida
            for item in lista_municipios:
                codigo_municipio = str(item.get('codigo_municipio', ''))
                if codigo_municipio:
                    self.dados_macrorregiao[codigo_municipio] = {
                        'codigo_regiao_pais': item.get('codigo_regiao_pais'),
                        'regiao_pais': item.get('regiao_pais'),
                        'codigo_uf': item.get('codigo_uf'),
                        'uf': item.get('uf'),
                        'codigo_macrorregiao_saude': item.get('codigo_macrorregiao_saude'),
                        'macrorregiao_saude': item.get('macrorregiao_saude'),
                        'codigo_regiao_saude': item.get('codigo_regiao_saude'),
                        'regiao_saude': item.get('regiao_saude'),
                        'municipio': item.get('municipio'),
                        'populacao_estimada_ibge_2022': item.get('populacao_estimada_ibge_2022')
                    }
            
            logging.info(f"‚úÖ Carregados dados de {len(self.dados_macrorregiao)} munic√≠pios")
            
        except Exception as e:
            logging.error(f"‚ùå Erro ao carregar dados de macrorregi√£o: {e}")
            raise
    
    def mesclar_dados_unidade(self, unidade_saude: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mescla os dados de uma unidade de sa√∫de com os dados de macrorregi√£o
        
        Args:
            unidade_saude (Dict[str, Any]): Dados da unidade de sa√∫de obtidos da API
            
        Returns:
            Dict[str, Any]: Unidade de sa√∫de com dados de macrorregi√£o mesclados
        """
        # Cria uma c√≥pia da unidade para n√£o modificar o original
        unidade_mesclada = unidade_saude.copy()
        
        # Remove o campo _metadata se existir (n√£o incluir no resultado final)
        if '_metadata' in unidade_mesclada:
            del unidade_mesclada['_metadata']
        
        # Extrai o c√≥digo do munic√≠pio da unidade de sa√∫de
        codigo_municipio = str(unidade_saude.get('codigo_municipio', ''))
        
        if codigo_municipio and codigo_municipio in self.dados_macrorregiao:
            # Obt√©m os dados de macrorregi√£o
            dados_macro_originais = self.dados_macrorregiao[codigo_municipio]
            
            # Cria uma c√≥pia dos dados de macrorregi√£o para evitar duplica√ß√µes
            dados_macro_limpos = dados_macro_originais.copy()
            
            # Remove campos duplicados que j√° existem no estabelecimento
            # O codigo_uf j√° est√° presente no estabelecimento, n√£o precisa duplicar
            if 'codigo_uf' in dados_macro_limpos:
                del dados_macro_limpos['codigo_uf']
            
            # Adiciona os dados de macrorregi√£o limpos em uma se√ß√£o espec√≠fica
            unidade_mesclada['dados_macrorregiao'] = dados_macro_limpos
            
            self.logger.info(f"‚úÖ Mesclagem bem-sucedida para c√≥digo munic√≠pio: {codigo_municipio}")
            
        else:
            # Caso n√£o encontre o c√≥digo do munic√≠pio
            unidade_mesclada['dados_macrorregiao'] = None
            
            self.logger.warning(f"‚ö†Ô∏è C√≥digo munic√≠pio n√£o encontrado: {codigo_municipio}")
        
        return unidade_mesclada
    
    def mesclar_arquivo_resultados(self, arquivo_entrada: str, arquivo_saida: str):
        """
        Mescla um arquivo completo de resultados da API CNES com dados de macrorregi√£o
        
        Args:
            arquivo_entrada (str): Caminho para o arquivo JSON com resultados da API CNES
            arquivo_saida (str): Caminho para salvar o arquivo mesclado
        """
        try:
            logging.info(f"üîÑ Iniciando mesclagem de arquivo: {arquivo_entrada}")
            
            # Carrega os dados do arquivo de entrada
            with open(arquivo_entrada, 'r', encoding='utf-8') as arquivo:
                dados_entrada = json.load(arquivo)
            
            # Verifica se √© um arquivo de resultados do automatizador
            if 'estabelecimentos' in dados_entrada:
                estabelecimentos = dados_entrada['estabelecimentos']
            elif isinstance(dados_entrada, list):
                estabelecimentos = dados_entrada
            else:
                raise ValueError("Estrutura do arquivo de entrada n√£o reconhecida")
            
            # Estat√≠sticas da mesclagem
            stats_mesclagem = {
                'total_unidades': len(estabelecimentos),
                'mesclagens_bem_sucedidas': 0,
                'mesclagens_falharam': 0,
                'codigos_municipio_nao_encontrados': []
            }
            
            # Inicializa tracker de progresso para mesclagem
            progress_tracker = ProgressTracker(len(estabelecimentos), "üó∫Ô∏è Mesclando macrorregi√£o")
            
            # Processa cada estabelecimento
            estabelecimentos_mesclados = []
            for i, estabelecimento in enumerate(estabelecimentos, 1):
                # Mescla os dados
                estabelecimento_mesclado = self.mesclar_dados_unidade(estabelecimento)
                
                # Atualiza estat√≠sticas
                if estabelecimento_mesclado.get('dados_macrorregiao') is not None:
                    stats_mesclagem['mesclagens_bem_sucedidas'] += 1
                else:
                    stats_mesclagem['mesclagens_falharam'] += 1
                    codigo_municipio = str(estabelecimento.get('codigo_municipio', ''))
                    if codigo_municipio:
                        stats_mesclagem['codigos_municipio_nao_encontrados'].append(codigo_municipio)
                
                estabelecimentos_mesclados.append(estabelecimento_mesclado)
                
                # Atualiza progresso a cada 50 estabelecimentos
                if i % 50 == 0 or i == len(estabelecimentos):
                    progress_tracker.update(i)
            
            # Finaliza progresso
            progress_tracker.finish()
            
            # Prepara dados de sa√≠da
            dados_saida = {
                'metadados_mesclagem': {
                    'data_mesclagem': datetime.now().isoformat(),
                    'arquivo_entrada': arquivo_entrada,
                    'arquivo_macrorregiao': self.arquivo_macrorregiao,
                    'versao_merger': '1.0_with_progress',
                    'estatisticas': stats_mesclagem
                },
                'estabelecimentos_com_macrorregiao': estabelecimentos_mesclados
            }
            
            # Preserva metadados originais se existirem
            if 'metadados' in dados_entrada:
                dados_saida['metadados_originais'] = dados_entrada['metadados']
            
            # Preserva erros originais se existirem
            if 'erros' in dados_entrada:
                dados_saida['erros_originais'] = dados_entrada['erros']
            
            # Salva o arquivo mesclado
            with open(arquivo_saida, 'w', encoding='utf-8') as arquivo:
                json.dump(dados_saida, arquivo, ensure_ascii=False, indent=2, cls=DateTimeEncoder)
            
            # Verifica se o arquivo foi salvo corretamente
            tamanho_arquivo = os.path.getsize(arquivo_saida)
            
            logging.info(f"‚úÖ Mesclagem conclu√≠da com sucesso!")
            logging.info(f"üìÅ Arquivo mesclado salvo: {arquivo_saida}")
            logging.info(f"üìä Tamanho do arquivo: {tamanho_arquivo:,} bytes")
            logging.info(f"üè• Total de unidades processadas: {stats_mesclagem['total_unidades']}")
            logging.info(f"‚úÖ Mesclagens bem-sucedidas: {stats_mesclagem['mesclagens_bem_sucedidas']}")
            logging.info(f"‚ùå Mesclagens falharam: {stats_mesclagem['mesclagens_falharam']}")
            logging.info(f"üìà Taxa de sucesso: {(stats_mesclagem['mesclagens_bem_sucedidas']/stats_mesclagem['total_unidades']*100):.1f}%")
            
            return dados_saida
            
        except Exception as e:
            logging.error(f"‚ùå Erro durante mesclagem: {e}")
            raise

def main():
    """
    Fun√ß√£o principal do script - Processamento integrado ASS√çNCRONO de c√≥digos CNES com mesclagem de macrorregi√£o
    """
    print("üè• AUTOMATIZADOR DA API CNES - VERS√ÉO ASS√çNCRONA OTIMIZADA")
    print("üîó Consulta detalhada por c√≥digo de estabelecimento")
    print("üõ†Ô∏è Corre√ß√µes: Processamento paralelo, otimiza√ß√µes de velocidade")
    print("üó∫Ô∏è FUNCIONALIDADE: Processamento r√°pido e mesclagem com dados de macrorregi√£o")
    print("‚ö° NOVO: Requisi√ß√µes simult√¢neas com loading em tempo real!")
    print("üìä NOVO: Barra de progresso, ETA e velocidade em tempo real!")
    print("=" * 80)
    
    # Solicita arquivo de entrada
    arquivo_entrada = input("\nDigite o caminho do arquivo JSON com os c√≥digos CNES: ").strip()
    
    if not os.path.exists(arquivo_entrada):
        print(f"‚ùå Arquivo n√£o encontrado: {arquivo_entrada}")
        return
    
    # Configura√ß√µes de performance
    print("\n‚ö° CONFIGURA√á√ïES DE PERFORMANCE:")
    concurrent_str = input("N√∫mero de requisi√ß√µes simult√¢neas (padr√£o: 15, m√°x recomendado: 25): ").strip()
    concurrent_requests = int(concurrent_str) if concurrent_str else 15
    
    delay_str = input("Delay entre lotes em segundos (padr√£o: 0.3): ").strip()
    delay_between_batches = float(delay_str) if delay_str else 0.3
    
    # Valida√ß√£o das configura√ß√µes
    if concurrent_requests > 25:
        print("‚ö†Ô∏è Aviso: Mais de 25 requisi√ß√µes simult√¢neas pode sobrecarregar a API")
        confirma_config = input("Deseja continuar mesmo assim? (s/n): ").strip().lower()
        if confirma_config != 's':
            concurrent_requests = 15
            print("‚úÖ Configura√ß√£o ajustada para 15 requisi√ß√µes simult√¢neas")
    
    # Verifica arquivo de macrorregi√£o
    arquivo_macrorregiao = input("\nDigite o caminho do arquivo de macrorregi√£o (ou pressione Enter para usar o padr√£o): ").strip()
    
    if not arquivo_macrorregiao:
        caminhos_padrao = [
            'macrorregiao_regiao_saude_municipios.json',
            '../macrorregiao_regiao_saude_municipios.json',
            os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'macrorregiao_regiao_saude_municipios.json')
        ]
        
        for caminho in caminhos_padrao:
            if os.path.exists(caminho):
                arquivo_macrorregiao = caminho
                break
        
        if not arquivo_macrorregiao:
            print("‚ùå Arquivo de macrorregi√£o n√£o encontrado. Especifique o caminho completo.")
            return
    
    if not os.path.exists(arquivo_macrorregiao):
        print(f"‚ùå Arquivo de macrorregi√£o n√£o encontrado: {arquivo_macrorregiao}")
        return
    
    async def processar_async():
        try:
            # Inicializa o automatizador ass√≠ncrono
            automatizador = CNESAPIAutomator(
                concurrent_requests=concurrent_requests,
                delay_between_batches=delay_between_batches
            )
            
            # Carrega os c√≥digos CNES
            codigos = automatizador.carregar_codigos_cnes(arquivo_entrada)
            
            # Calcula tempo estimado (muito mais r√°pido agora!)
            tempo_estimado = (len(codigos) / concurrent_requests) * delay_between_batches
            tempo_estimado += len(codigos) * 0.1  # Overhead estimado por requisi√ß√£o
            
            # Confirma antes de processar
            print(f"\nüìã Encontrados {len(codigos)} c√≥digos CNES para processar")
            print(f"‚ö° Configura√ß√£o: {concurrent_requests} requisi√ß√µes simult√¢neas")
            print(f"‚è±Ô∏è Tempo estimado: {tempo_estimado:.1f} segundos ({tempo_estimado/60:.1f} minutos)")
            print(f"üöÄ Velocidade estimada: ~{len(codigos)/tempo_estimado:.1f} requisi√ß√µes/segundo")
            
            confirma = input("\nDeseja continuar? (s/n): ").strip().lower()
            
            if confirma == 's':
                print(f"\nüöÄ Iniciando processamento ass√≠ncrono otimizado...")
                print(f"üìä Loading em tempo real com ETA ativado!")
                print()
                
                # Processa os c√≥digos de forma ass√≠ncrona
                resultados = await automatizador.processar_lista_codigos(codigos)
                
                # Salva os resultados iniciais
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                arquivo_intermediario = f"cnes_resultados_temp_{timestamp}.json"
                automatizador.salvar_resultados(resultados, arquivo_intermediario)
                
                print(f"\n‚úÖ Processamento da API conclu√≠do!")
                print(f"üîÑ Iniciando mesclagem com dados de macrorregi√£o...")
                print()
                
                # Inicializa o merger
                merger = CNESMacrorregiaeMerger(arquivo_macrorregiao)
                
                # Executa a mesclagem
                arquivo_final = f"cnes_com_macrorregiao_{timestamp}.json"
                resultado_final = merger.mesclar_arquivo_resultados(arquivo_intermediario, arquivo_final)
                
                # Remove arquivo intermedi√°rio
                try:
                    os.remove(arquivo_intermediario)
                    print(f"üóëÔ∏è Arquivo tempor√°rio removido: {arquivo_intermediario}")
                except:
                    pass
                
                print(f"\nüéâ Processamento e mesclagem conclu√≠dos!")
                print(f"üìÅ Arquivo final: {arquivo_final}")
                
                # Mostra estat√≠sticas finais
                stats_api = resultados['resumo']
                stats_mesclagem = resultado_final['metadados_mesclagem']['estatisticas']
                
                print(f"\nüìä Estat√≠sticas finais:")
                print(f"   - C√≥digos processados: {stats_api['total_sucessos']}")
                print(f"   - Mesclagens bem-sucedidas: {stats_mesclagem['mesclagens_bem_sucedidas']}")
                print(f"   - Taxa de sucesso API: {stats_api['taxa_sucesso']}")
                print(f"   - Taxa de sucesso mesclagem: {(stats_mesclagem['mesclagens_bem_sucedidas']/stats_mesclagem['total_unidades']*100):.1f}%")
                print(f"   - Velocidade m√©dia: {stats_api['velocidade_media']}")
                print(f"   - Tempo total: {resultados['metadados']['tempo_execucao_segundos']:.1f} segundos")
                
        except Exception as e:
            logging.error(f"‚ùå Erro durante processamento integrado ass√≠ncrono: {e}")
            print(f"‚ùå Erro: {e}")
    
    # Executa o processamento ass√≠ncrono
    try:
        asyncio.run(processar_async())
    except KeyboardInterrupt:
        print("\n‚ùå Processamento interrompido pelo usu√°rio")
    except Exception as e:
        logging.error(f"‚ùå Erro ao executar processamento ass√≠ncrono: {e}")
        print(f"‚ùå Erro: {e}")

if __name__ == "__main__":
    main()
